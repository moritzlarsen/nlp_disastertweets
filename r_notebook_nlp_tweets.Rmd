---
title: "R Notebook NLP with Disaster Tweets"
output: html_notebook
---
This is a R Notebook for plotting target distribution, keywords, word counts and bigrams of the Kaggle Dataset for the Challenge NLP with Disaster Tweets (https://www.kaggle.com/c/nlp-getting-started).

First we are loading the required packages
```{r}
library(dplyr)
library(readr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tidyr)
```
Next we are loading the train and test set.
```{r}
train_tweets <- read_csv("https://raw.githubusercontent.com/moritzlarsen/nlp_disastertweets/master/train.csv")
test_tweets <- read_csv("https://raw.githubusercontent.com/moritzlarsen/nlp_disastertweets/master/test.csv")
```
Text cleaning
```{r}
tidy_tweets <- train_tweets[c(-1,-3)]

remove_reg <- "&amp;|&lt;|&gt;"
url_reg <- "https?:\\/\\/t.co\\/[A-Za-z0-9]+"


tidy_tweets <- tidy_tweets %>%
  mutate(text = str_remove_all(text, remove_reg),
         text = str_remove_all(text, url_reg),
         text = str_replace_all(text, "@|#|\\!|\\?|\\+|&|\\*|\\[|\\]|-|%|\\.|\\:|\\/|\\(|\\)|;|\\$|=|\\>|\\<|\\||\\{|\\}|\\^|\\n| '|' |^'|'$|,|_|÷", " "),
         text = tolower(text),
         text = str_replace_all(text, "û|ó|ò|å|ê|ã|¢|ï|ª", " "),
         text = str_remove_all(text, "\u0089|\u009d"),
         text = str_replace_all(text, "[:digit:]", ""),
         text = str_replace_all(text, " +", " "),
         text = str_remove_all(text, "^ | $"))
```

Plotting the target distribution.
```{r}
ggplot(tidy_tweets)+
  aes(factor(target,labels = c("Non-Disaster","Disaster")), fill = factor(target))+
  geom_bar(stat = "count", width = 0.5)+
  geom_text(aes(label=..count..), stat = "count", position = position_stack(0.95))+
  scale_fill_brewer(palette = "Accent")+
  ggtitle("Target count in Training Set")+
  xlab("target") +
  theme_light()+
  theme(legend.position = "none")
```

Plotting the most common keywords in Disaster and No-Disaster Tweets

Disaster Tweets
```{r}
keyword_dis <- as.data.frame(head(sort(table(tidy_tweets$keyword[tidy_tweets$target==1]), decreasing = T), n=20), 
                              stringsAsFactors = F)
colnames(keyword_dis) = c("keyword", "frequency")

keyword_dis %>%
  mutate(keyword = fct_reorder(keyword, frequency)) %>%
  ggplot(aes(x=keyword, y=frequency)) +
  geom_bar(stat="identity", fill="#BEAED4", alpha=0.9, width=.4) +
  coord_flip() +
  ggtitle("Disaster Tweets")+
  xlab("keyword") +
  theme_light()
```

No-Disaster Tweets
```{r}
keyword_nodis <- as.data.frame(head(sort(table(tidy_tweets$keyword[tidy_tweets$target==0]), decreasing = T), n=20), 
                              stringsAsFactors = F)
colnames(keyword_nodis) = c("keyword", "frequency")

keyword_nodis %>%
  mutate(keyword = fct_reorder(keyword, frequency)) %>%
  ggplot(aes(x=keyword, y=frequency)) +
  geom_bar(stat="identity", fill="#7FC97F", alpha=0.9, width=.4) +
  coord_flip() +
  ggtitle("Non-Disaster Tweets")+
  xlab("keyword") +
  theme_bw()
```

Plotting the most common words in Disaster and No-Disaster Tweets

Disaster Tweets
```{r}
#load stopwords
data("stop_words")

words_dis <- tidy_tweets[tidy_tweets$target==1,] %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE) %>%
  anti_join(stop_words)
  
words_dis %>%
  slice(1:15) %>%
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat="identity", fill="#BEAED4", alpha=0.9, width=.6) +
  coord_flip() +
  ggtitle("Disaster Tweets")+
  xlab("word") +
  theme_light()
```

No-Disaster Tweets
```{r}
words_nodis <- tidy_tweets[tidy_tweets$target==0,] %>%
  unnest_tokens(word, text) %>%
  count(word, sort = TRUE) %>%
  anti_join(stop_words)

words_nodis %>%
  slice(1:15) %>%
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat="identity", fill="#7FC97F", alpha=0.9, width=.4) +
  coord_flip() +
  ggtitle("Non-Disaster Tweets")+
  xlab("word") +
  theme_bw()
```

Plotting the most common bigrams in Disaster and No-Disaster Tweets

Disaster Tweets
```{r}
bigrams_dis <- tidy_tweets[tidy_tweets$target==1,] %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE)

bigrams_dis %>%
  unite(bigram, word1, word2, sep = " ") %>%
  slice(1:15) %>%
  mutate(bigram = fct_reorder(bigram, n)) %>%
  ggplot(aes(bigram, n)) +
  geom_bar(stat="identity", fill="#BEAED4", alpha=0.9, width=.6) +
  coord_flip() +
  ggtitle("Disaster Bigrams")+
  xlab("bigram") +
  theme_light()
```

No-Disaster Tweets
```{r}
bigrams_nodis <- tidy_tweets[tidy_tweets$target==0,] %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  count(word1, word2, sort = TRUE)

bigrams_nodis %>%
  unite(bigram, word1, word2, sep = " ") %>%
  slice(1:15) %>%
  mutate(bigram = fct_reorder(bigram, n)) %>%
  ggplot(aes(bigram, n)) +
  geom_bar(stat="identity", fill="#7FC97F", alpha=0.9, width=.6) +
  coord_flip() +
  ggtitle("Non-Disaster Bigrams")+
  xlab("bigram") +
  theme_light()
```

